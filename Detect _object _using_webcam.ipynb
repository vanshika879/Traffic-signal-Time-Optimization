{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ecdc2-6f9c-4e89-ae9d-3daade4eb603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x800 (no detections), 356.4ms\n",
      "Speed: 6.8ms preprocess, 356.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 389.3ms\n",
      "Speed: 15.2ms preprocess, 389.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 382.2ms\n",
      "Speed: 9.0ms preprocess, 382.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 379.4ms\n",
      "Speed: 12.0ms preprocess, 379.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 345.1ms\n",
      "Speed: 11.2ms preprocess, 345.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 334.7ms\n",
      "Speed: 12.6ms preprocess, 334.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 314.4ms\n",
      "Speed: 18.0ms preprocess, 314.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 352.8ms\n",
      "Speed: 4.0ms preprocess, 352.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 319.4ms\n",
      "Speed: 18.6ms preprocess, 319.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 327.8ms\n",
      "Speed: 9.0ms preprocess, 327.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 342.8ms\n",
      "Speed: 12.7ms preprocess, 342.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 335.1ms\n",
      "Speed: 11.6ms preprocess, 335.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 328.8ms\n",
      "Speed: 10.9ms preprocess, 328.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 351.8ms\n",
      "Speed: 18.9ms preprocess, 351.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 322.5ms\n",
      "Speed: 11.3ms preprocess, 322.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 355.6ms\n",
      "Speed: 11.0ms preprocess, 355.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 359.6ms\n",
      "Speed: 15.6ms preprocess, 359.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 358.6ms\n",
      "Speed: 19.2ms preprocess, 358.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 323.0ms\n",
      "Speed: 14.7ms preprocess, 323.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 347.4ms\n",
      "Speed: 16.8ms preprocess, 347.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 355.0ms\n",
      "Speed: 14.8ms preprocess, 355.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 338.3ms\n",
      "Speed: 2.2ms preprocess, 338.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 359.1ms\n",
      "Speed: 9.7ms preprocess, 359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 327.2ms\n",
      "Speed: 12.6ms preprocess, 327.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 313.3ms\n",
      "Speed: 10.9ms preprocess, 313.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 348.2ms\n",
      "Speed: 13.3ms preprocess, 348.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 339.6ms\n",
      "Speed: 9.4ms preprocess, 339.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 324.2ms\n",
      "Speed: 12.5ms preprocess, 324.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 341.2ms\n",
      "Speed: 17.6ms preprocess, 341.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 325.0ms\n",
      "Speed: 14.2ms preprocess, 325.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 339.3ms\n",
      "Speed: 6.6ms preprocess, 339.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 341.5ms\n",
      "Speed: 9.0ms preprocess, 341.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 342.6ms\n",
      "Speed: 11.0ms preprocess, 342.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 Yulu Bike, 327.5ms\n",
      "Speed: 11.7ms preprocess, 327.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.27\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 340.9ms\n",
      "Speed: 15.1ms preprocess, 340.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.51\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 334.8ms\n",
      "Speed: 15.6ms preprocess, 334.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.52\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 (no detections), 348.2ms\n",
      "Speed: 3.1ms preprocess, 348.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 Yulu Bike, 323.9ms\n",
      "Speed: 8.0ms preprocess, 323.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.35\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 348.0ms\n",
      "Speed: 8.6ms preprocess, 348.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.41\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 2 Yulu Bikes, 348.8ms\n",
      "Speed: 9.7ms preprocess, 348.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.48\n",
      "Class name --> motorbike\n",
      "Confidence ---> 0.32\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 347.4ms\n",
      "Speed: 10.1ms preprocess, 347.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.63\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 340.6ms\n",
      "Speed: 11.3ms preprocess, 340.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.7\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 329.6ms\n",
      "Speed: 14.0ms preprocess, 329.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.78\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 320.8ms\n",
      "Speed: 10.9ms preprocess, 320.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.83\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 342.9ms\n",
      "Speed: 8.4ms preprocess, 342.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.61\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 349.0ms\n",
      "Speed: 10.0ms preprocess, 349.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.79\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 2 Yulu Bikes, 314.1ms\n",
      "Speed: 13.6ms preprocess, 314.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.7\n",
      "Class name --> motorbike\n",
      "Confidence ---> 0.3\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 2 Yulu Bikes, 354.6ms\n",
      "Speed: 8.2ms preprocess, 354.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.82\n",
      "Class name --> motorbike\n",
      "Confidence ---> 0.36\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 332.7ms\n",
      "Speed: 5.6ms preprocess, 332.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.66\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 331.6ms\n",
      "Speed: 8.3ms preprocess, 331.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.86\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 330.7ms\n",
      "Speed: 7.3ms preprocess, 330.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.88\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 329.7ms\n",
      "Speed: 8.2ms preprocess, 329.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.88\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 345.7ms\n",
      "Speed: 6.0ms preprocess, 345.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.83\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 350.1ms\n",
      "Speed: 2.2ms preprocess, 350.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.87\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 329.9ms\n",
      "Speed: 12.2ms preprocess, 329.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.8\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 326.9ms\n",
      "Speed: 12.3ms preprocess, 326.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.78\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 318.2ms\n",
      "Speed: 8.3ms preprocess, 318.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.81\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 348.7ms\n",
      "Speed: 12.0ms preprocess, 348.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.91\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 2 Yulu Bikes, 344.9ms\n",
      "Speed: 11.9ms preprocess, 344.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.8\n",
      "Class name --> motorbike\n",
      "Confidence ---> 0.32\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 2 Yulu Bikes, 338.2ms\n",
      "Speed: 8.1ms preprocess, 338.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.77\n",
      "Class name --> motorbike\n",
      "Confidence ---> 0.49\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 349.7ms\n",
      "Speed: 6.5ms preprocess, 349.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.62\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 339.1ms\n",
      "Speed: 12.2ms preprocess, 339.1ms inference, 15.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.61\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 345.3ms\n",
      "Speed: 15.7ms preprocess, 345.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.75\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 334.7ms\n",
      "Speed: 10.3ms preprocess, 334.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.52\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 339.0ms\n",
      "Speed: 4.3ms preprocess, 339.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.31\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 338.0ms\n",
      "Speed: 10.9ms preprocess, 338.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.46\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 1 Yulu Bike, 346.6ms\n",
      "Speed: 12.2ms preprocess, 346.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.46\n",
      "Class name --> motorbike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 (no detections), 344.9ms\n",
      "Speed: 4.8ms preprocess, 344.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 329.5ms\n",
      "Speed: 9.2ms preprocess, 329.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 342.8ms\n",
      "Speed: 11.0ms preprocess, 342.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 344.8ms\n",
      "Speed: 9.3ms preprocess, 344.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 326.6ms\n",
      "Speed: 10.2ms preprocess, 326.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 332.5ms\n",
      "Speed: 5.5ms preprocess, 332.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 328.6ms\n",
      "Speed: 15.0ms preprocess, 328.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 336.2ms\n",
      "Speed: 11.6ms preprocess, 336.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 332.8ms\n",
      "Speed: 10.7ms preprocess, 332.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 329.8ms\n",
      "Speed: 11.0ms preprocess, 329.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 320.7ms\n",
      "Speed: 11.0ms preprocess, 320.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 358.0ms\n",
      "Speed: 6.0ms preprocess, 358.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 347.3ms\n",
      "Speed: 12.7ms preprocess, 347.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 348.4ms\n",
      "Speed: 11.7ms preprocess, 348.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 343.2ms\n",
      "Speed: 0.0ms preprocess, 343.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 353.0ms\n",
      "Speed: 5.4ms preprocess, 353.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 481.1ms\n",
      "Speed: 11.6ms preprocess, 481.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 431.7ms\n",
      "Speed: 12.2ms preprocess, 431.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 425.8ms\n",
      "Speed: 12.0ms preprocess, 425.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 744.8ms\n",
      "Speed: 11.6ms preprocess, 744.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 373.8ms\n",
      "Speed: 7.7ms preprocess, 373.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 326.7ms\n",
      "Speed: 7.1ms preprocess, 326.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 326.7ms\n",
      "Speed: 1.3ms preprocess, 326.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 341.5ms\n",
      "Speed: 3.2ms preprocess, 341.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 324.0ms\n",
      "Speed: 4.0ms preprocess, 324.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 323.2ms\n",
      "Speed: 5.4ms preprocess, 323.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 334.9ms\n",
      "Speed: 8.3ms preprocess, 334.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 331.0ms\n",
      "Speed: 8.8ms preprocess, 331.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 340.2ms\n",
      "Speed: 3.6ms preprocess, 340.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 338.0ms\n",
      "Speed: 6.4ms preprocess, 338.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 341.1ms\n",
      "Speed: 4.4ms preprocess, 341.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 336.9ms\n",
      "Speed: 5.0ms preprocess, 336.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 319.8ms\n",
      "Speed: 4.0ms preprocess, 319.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 322.1ms\n",
      "Speed: 10.9ms preprocess, 322.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 337.2ms\n",
      "Speed: 8.1ms preprocess, 337.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 334.4ms\n",
      "Speed: 8.0ms preprocess, 334.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 321.3ms\n",
      "Speed: 6.7ms preprocess, 321.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 323.0ms\n",
      "Speed: 10.6ms preprocess, 323.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 337.4ms\n",
      "Speed: 4.3ms preprocess, 337.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 379.6ms\n",
      "Speed: 5.2ms preprocess, 379.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 342.9ms\n",
      "Speed: 5.0ms preprocess, 342.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 355.0ms\n",
      "Speed: 4.3ms preprocess, 355.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 332.8ms\n",
      "Speed: 4.9ms preprocess, 332.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 350.2ms\n",
      "Speed: 3.9ms preprocess, 350.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 357.7ms\n",
      "Speed: 6.5ms preprocess, 357.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 342.3ms\n",
      "Speed: 5.2ms preprocess, 342.3ms inference, 8.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 422.6ms\n",
      "Speed: 5.2ms preprocess, 422.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 452.1ms\n",
      "Speed: 5.2ms preprocess, 452.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 522.5ms\n",
      "Speed: 9.2ms preprocess, 522.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 502.6ms\n",
      "Speed: 14.4ms preprocess, 502.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 441.8ms\n",
      "Speed: 12.8ms preprocess, 441.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 416.7ms\n",
      "Speed: 0.0ms preprocess, 416.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 444.9ms\n",
      "Speed: 2.7ms preprocess, 444.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 656.6ms\n",
      "Speed: 7.4ms preprocess, 656.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 774.7ms\n",
      "Speed: 6.5ms preprocess, 774.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 698.0ms\n",
      "Speed: 24.7ms preprocess, 698.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 630.8ms\n",
      "Speed: 25.9ms preprocess, 630.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 408.1ms\n",
      "Speed: 0.0ms preprocess, 408.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 368.4ms\n",
      "Speed: 4.2ms preprocess, 368.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 464.5ms\n",
      "Speed: 0.0ms preprocess, 464.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 563.4ms\n",
      "Speed: 3.1ms preprocess, 563.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x800 (no detections), 526.4ms\n",
      "Speed: 0.0ms preprocess, 526.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 528.1ms\n",
      "Speed: 15.2ms preprocess, 528.1ms inference, 12.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 462.3ms\n",
      "Speed: 8.0ms preprocess, 462.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 648.3ms\n",
      "Speed: 10.8ms preprocess, 648.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 353.5ms\n",
      "Speed: 6.6ms preprocess, 353.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 426.6ms\n",
      "Speed: 0.0ms preprocess, 426.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 621.5ms\n",
      "Speed: 11.7ms preprocess, 621.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 522.4ms\n",
      "Speed: 13.1ms preprocess, 522.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 332.6ms\n",
      "Speed: 0.0ms preprocess, 332.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 332.1ms\n",
      "Speed: 0.0ms preprocess, 332.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 331.9ms\n",
      "Speed: 0.0ms preprocess, 331.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 427.9ms\n",
      "Speed: 0.0ms preprocess, 427.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 640.3ms\n",
      "Speed: 5.2ms preprocess, 640.3ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 540.4ms\n",
      "Speed: 19.0ms preprocess, 540.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 560.1ms\n",
      "Speed: 0.0ms preprocess, 560.1ms inference, 7.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 403.0ms\n",
      "Speed: 12.7ms preprocess, 403.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 463.1ms\n",
      "Speed: 4.1ms preprocess, 463.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 402.6ms\n",
      "Speed: 3.0ms preprocess, 402.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 393.6ms\n",
      "Speed: 7.3ms preprocess, 393.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 357.6ms\n",
      "Speed: 0.0ms preprocess, 357.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 332.5ms\n",
      "Speed: 0.0ms preprocess, 332.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 764.9ms\n",
      "Speed: 0.0ms preprocess, 764.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 591.5ms\n",
      "Speed: 5.4ms preprocess, 591.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 408.9ms\n",
      "Speed: 5.3ms preprocess, 408.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 485.8ms\n",
      "Speed: 4.1ms preprocess, 485.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 744.2ms\n",
      "Speed: 11.1ms preprocess, 744.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 365.2ms\n",
      "Speed: 5.2ms preprocess, 365.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 354.9ms\n",
      "Speed: 0.0ms preprocess, 354.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 375.2ms\n",
      "Speed: 0.0ms preprocess, 375.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 515.8ms\n",
      "Speed: 23.6ms preprocess, 515.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 462.8ms\n",
      "Speed: 15.1ms preprocess, 462.8ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 372.5ms\n",
      "Speed: 0.0ms preprocess, 372.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 503.3ms\n",
      "Speed: 11.9ms preprocess, 503.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 449.6ms\n",
      "Speed: 8.6ms preprocess, 449.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 376.7ms\n",
      "Speed: 0.0ms preprocess, 376.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 347.8ms\n",
      "Speed: 0.0ms preprocess, 347.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 333.6ms\n",
      "Speed: 0.0ms preprocess, 333.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 332.6ms\n",
      "Speed: 0.0ms preprocess, 332.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 347.5ms\n",
      "Speed: 0.0ms preprocess, 347.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 318.2ms\n",
      "Speed: 0.0ms preprocess, 318.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 333.4ms\n",
      "Speed: 0.0ms preprocess, 333.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 428.7ms\n",
      "Speed: 0.0ms preprocess, 428.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 412.3ms\n",
      "Speed: 25.6ms preprocess, 412.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 364.9ms\n",
      "Speed: 0.0ms preprocess, 364.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 364.6ms\n",
      "Speed: 0.0ms preprocess, 364.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 370.2ms\n",
      "Speed: 0.0ms preprocess, 370.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 391.0ms\n",
      "Speed: 0.0ms preprocess, 391.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 446.2ms\n",
      "Speed: 4.3ms preprocess, 446.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 466.9ms\n",
      "Speed: 9.2ms preprocess, 466.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 381.4ms\n",
      "Speed: 0.0ms preprocess, 381.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 429.0ms\n",
      "Speed: 12.5ms preprocess, 429.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 618.7ms\n",
      "Speed: 8.0ms preprocess, 618.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 350.9ms\n",
      "Speed: 10.4ms preprocess, 350.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 372.6ms\n",
      "Speed: 9.2ms preprocess, 372.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 321.8ms\n",
      "Speed: 3.3ms preprocess, 321.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 313.2ms\n",
      "Speed: 10.4ms preprocess, 313.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 323.9ms\n",
      "Speed: 7.4ms preprocess, 323.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 316.5ms\n",
      "Speed: 4.0ms preprocess, 316.5ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 335.0ms\n",
      "Speed: 5.3ms preprocess, 335.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 322.4ms\n",
      "Speed: 4.7ms preprocess, 322.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 332.5ms\n",
      "Speed: 5.0ms preprocess, 332.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x800 (no detections), 328.7ms\n",
      "Speed: 4.0ms preprocess, 328.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 323.8ms\n",
      "Speed: 4.1ms preprocess, 323.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 338.2ms\n",
      "Speed: 5.0ms preprocess, 338.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 315.0ms\n",
      "Speed: 6.1ms preprocess, 315.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 332.9ms\n",
      "Speed: 6.2ms preprocess, 332.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 319.4ms\n",
      "Speed: 5.0ms preprocess, 319.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 329.3ms\n",
      "Speed: 3.0ms preprocess, 329.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 316.6ms\n",
      "Speed: 10.2ms preprocess, 316.6ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 312.4ms\n",
      "Speed: 6.0ms preprocess, 312.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 312.7ms\n",
      "Speed: 5.1ms preprocess, 312.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 344.5ms\n",
      "Speed: 4.4ms preprocess, 344.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 339.5ms\n",
      "Speed: 4.4ms preprocess, 339.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 322.8ms\n",
      "Speed: 5.0ms preprocess, 322.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 339.0ms\n",
      "Speed: 10.0ms preprocess, 339.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 333.4ms\n",
      "Speed: 4.8ms preprocess, 333.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 327.7ms\n",
      "Speed: 5.0ms preprocess, 327.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 324.5ms\n",
      "Speed: 4.7ms preprocess, 324.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 347.7ms\n",
      "Speed: 6.0ms preprocess, 347.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 328.7ms\n",
      "Speed: 6.4ms preprocess, 328.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 329.3ms\n",
      "Speed: 8.8ms preprocess, 329.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 332.6ms\n",
      "Speed: 7.7ms preprocess, 332.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 320.8ms\n",
      "Speed: 6.1ms preprocess, 320.8ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 304.5ms\n",
      "Speed: 5.0ms preprocess, 304.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 313.7ms\n",
      "Speed: 4.0ms preprocess, 313.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 303.9ms\n",
      "Speed: 8.0ms preprocess, 303.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 327.1ms\n",
      "Speed: 7.0ms preprocess, 327.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 322.1ms\n",
      "Speed: 4.6ms preprocess, 322.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 320.4ms\n",
      "Speed: 4.0ms preprocess, 320.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 320.9ms\n",
      "Speed: 5.5ms preprocess, 320.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 325.8ms\n",
      "Speed: 0.0ms preprocess, 325.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 294.9ms\n",
      "Speed: 4.1ms preprocess, 294.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 308.8ms\n",
      "Speed: 8.5ms preprocess, 308.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 326.3ms\n",
      "Speed: 6.3ms preprocess, 326.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 306.8ms\n",
      "Speed: 3.0ms preprocess, 306.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 318.3ms\n",
      "Speed: 7.8ms preprocess, 318.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 316.9ms\n",
      "Speed: 5.0ms preprocess, 316.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 314.5ms\n",
      "Speed: 9.5ms preprocess, 314.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 320.7ms\n",
      "Speed: 5.2ms preprocess, 320.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 315.6ms\n",
      "Speed: 4.1ms preprocess, 315.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 314.8ms\n",
      "Speed: 4.0ms preprocess, 314.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 315.1ms\n",
      "Speed: 4.3ms preprocess, 315.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 315.2ms\n",
      "Speed: 7.7ms preprocess, 315.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 304.1ms\n",
      "Speed: 5.9ms preprocess, 304.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 304.9ms\n",
      "Speed: 6.3ms preprocess, 304.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 312.1ms\n",
      "Speed: 9.4ms preprocess, 312.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 320.4ms\n",
      "Speed: 6.0ms preprocess, 320.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 320.2ms\n",
      "Speed: 3.5ms preprocess, 320.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 314.3ms\n",
      "Speed: 11.0ms preprocess, 314.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 302.6ms\n",
      "Speed: 6.0ms preprocess, 302.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 321.4ms\n",
      "Speed: 6.0ms preprocess, 321.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 304.7ms\n",
      "Speed: 6.3ms preprocess, 304.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 305.3ms\n",
      "Speed: 5.3ms preprocess, 305.3ms inference, 15.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 306.2ms\n",
      "Speed: 6.0ms preprocess, 306.2ms inference, 15.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 320.9ms\n",
      "Speed: 4.2ms preprocess, 320.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 306.0ms\n",
      "Speed: 4.1ms preprocess, 306.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 315.9ms\n",
      "Speed: 4.1ms preprocess, 315.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 311.3ms\n",
      "Speed: 5.0ms preprocess, 311.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 326.2ms\n",
      "Speed: 0.0ms preprocess, 326.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 349.4ms\n",
      "Speed: 12.0ms preprocess, 349.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 354.8ms\n",
      "Speed: 5.0ms preprocess, 354.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 330.2ms\n",
      "Speed: 5.8ms preprocess, 330.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x800 (no detections), 421.2ms\n",
      "Speed: 5.0ms preprocess, 421.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 393.9ms\n",
      "Speed: 6.3ms preprocess, 393.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 395.6ms\n",
      "Speed: 10.6ms preprocess, 395.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 364.9ms\n",
      "Speed: 6.0ms preprocess, 364.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 389.7ms\n",
      "Speed: 5.5ms preprocess, 389.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 369.2ms\n",
      "Speed: 13.8ms preprocess, 369.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 388.3ms\n",
      "Speed: 14.0ms preprocess, 388.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 434.4ms\n",
      "Speed: 13.3ms preprocess, 434.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 351.9ms\n",
      "Speed: 12.8ms preprocess, 351.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 349.6ms\n",
      "Speed: 6.5ms preprocess, 349.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 486.5ms\n",
      "Speed: 9.0ms preprocess, 486.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 412.0ms\n",
      "Speed: 12.0ms preprocess, 412.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 363.6ms\n",
      "Speed: 10.4ms preprocess, 363.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 334.4ms\n",
      "Speed: 9.1ms preprocess, 334.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 342.3ms\n",
      "Speed: 10.9ms preprocess, 342.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 343.1ms\n",
      "Speed: 9.0ms preprocess, 343.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 338.7ms\n",
      "Speed: 9.6ms preprocess, 338.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 333.7ms\n",
      "Speed: 6.7ms preprocess, 333.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 366.2ms\n",
      "Speed: 10.3ms preprocess, 366.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 337.4ms\n",
      "Speed: 8.5ms preprocess, 337.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 348.6ms\n",
      "Speed: 12.8ms preprocess, 348.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 322.0ms\n",
      "Speed: 10.2ms preprocess, 322.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 349.5ms\n",
      "Speed: 1.0ms preprocess, 349.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 353.4ms\n",
      "Speed: 7.2ms preprocess, 353.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 345.2ms\n",
      "Speed: 8.6ms preprocess, 345.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 324.3ms\n",
      "Speed: 9.1ms preprocess, 324.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 325.8ms\n",
      "Speed: 11.5ms preprocess, 325.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 318.5ms\n",
      "Speed: 6.5ms preprocess, 318.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 326.6ms\n",
      "Speed: 5.5ms preprocess, 326.6ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 318.0ms\n",
      "Speed: 7.4ms preprocess, 318.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 330.9ms\n",
      "Speed: 9.4ms preprocess, 330.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 327.3ms\n",
      "Speed: 6.0ms preprocess, 327.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 323.2ms\n",
      "Speed: 11.5ms preprocess, 323.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 322.2ms\n",
      "Speed: 4.3ms preprocess, 322.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 Bicycle, 313.8ms\n",
      "Speed: 8.7ms preprocess, 313.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ---> 0.34\n",
      "Class name --> person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x800 (no detections), 323.5ms\n",
      "Speed: 8.2ms preprocess, 323.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 332.9ms\n",
      "Speed: 8.0ms preprocess, 332.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 303.1ms\n",
      "Speed: 9.8ms preprocess, 303.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 318.4ms\n",
      "Speed: 12.0ms preprocess, 318.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 326.1ms\n",
      "Speed: 5.1ms preprocess, 326.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 313.8ms\n",
      "Speed: 7.7ms preprocess, 313.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 330.1ms\n",
      "Speed: 10.4ms preprocess, 330.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 341.0ms\n",
      "Speed: 5.0ms preprocess, 341.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 324.5ms\n",
      "Speed: 10.2ms preprocess, 324.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 326.0ms\n",
      "Speed: 9.2ms preprocess, 326.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 322.5ms\n",
      "Speed: 10.5ms preprocess, 322.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 329.3ms\n",
      "Speed: 11.2ms preprocess, 329.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 319.9ms\n",
      "Speed: 8.3ms preprocess, 319.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 340.4ms\n",
      "Speed: 10.6ms preprocess, 340.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 297.4ms\n",
      "Speed: 7.0ms preprocess, 297.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 336.5ms\n",
      "Speed: 6.4ms preprocess, 336.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 336.9ms\n",
      "Speed: 7.6ms preprocess, 336.9ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 314.7ms\n",
      "Speed: 2.0ms preprocess, 314.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 319.4ms\n",
      "Speed: 6.0ms preprocess, 319.4ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 320.1ms\n",
      "Speed: 8.2ms preprocess, 320.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 323.3ms\n",
      "Speed: 3.9ms preprocess, 323.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 325.8ms\n",
      "Speed: 2.0ms preprocess, 325.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 296.8ms\n",
      "Speed: 8.6ms preprocess, 296.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 324.4ms\n",
      "Speed: 10.9ms preprocess, 324.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 319.4ms\n",
      "Speed: 7.0ms preprocess, 319.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 335.7ms\n",
      "Speed: 10.2ms preprocess, 335.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 307.6ms\n",
      "Speed: 7.6ms preprocess, 307.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 326.5ms\n",
      "Speed: 9.2ms preprocess, 326.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 328.9ms\n",
      "Speed: 8.7ms preprocess, 328.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 346.1ms\n",
      "Speed: 6.2ms preprocess, 346.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 313.5ms\n",
      "Speed: 10.1ms preprocess, 313.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 329.8ms\n",
      "Speed: 5.7ms preprocess, 329.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 324.6ms\n",
      "Speed: 7.4ms preprocess, 324.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 323.1ms\n",
      "Speed: 6.8ms preprocess, 323.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 313.3ms\n",
      "Speed: 7.3ms preprocess, 313.3ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 316.1ms\n",
      "Speed: 11.6ms preprocess, 316.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 337.7ms\n",
      "Speed: 7.0ms preprocess, 337.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 316.6ms\n",
      "Speed: 7.1ms preprocess, 316.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 318.8ms\n",
      "Speed: 10.8ms preprocess, 318.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 320.7ms\n",
      "Speed: 7.8ms preprocess, 320.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 323.3ms\n",
      "Speed: 6.9ms preprocess, 323.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 323.0ms\n",
      "Speed: 7.8ms preprocess, 323.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 313.6ms\n",
      "Speed: 4.5ms preprocess, 313.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 335.3ms\n",
      "Speed: 9.0ms preprocess, 335.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 313.3ms\n",
      "Speed: 5.3ms preprocess, 313.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 314.4ms\n",
      "Speed: 6.0ms preprocess, 314.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 306.2ms\n",
      "Speed: 9.9ms preprocess, 306.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 316.2ms\n",
      "Speed: 7.6ms preprocess, 316.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 328.9ms\n",
      "Speed: 11.7ms preprocess, 328.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 308.2ms\n",
      "Speed: 10.4ms preprocess, 308.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 347.4ms\n",
      "Speed: 8.0ms preprocess, 347.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 326.3ms\n",
      "Speed: 8.3ms preprocess, 326.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 320.4ms\n",
      "Speed: 7.8ms preprocess, 320.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 314.3ms\n",
      "Speed: 6.9ms preprocess, 314.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 305.8ms\n",
      "Speed: 9.6ms preprocess, 305.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 318.6ms\n",
      "Speed: 10.0ms preprocess, 318.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 313.2ms\n",
      "Speed: 10.4ms preprocess, 313.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 300.7ms\n",
      "Speed: 11.6ms preprocess, 300.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 322.7ms\n",
      "Speed: 8.4ms preprocess, 322.7ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 357.0ms\n",
      "Speed: 9.4ms preprocess, 357.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 373.9ms\n",
      "Speed: 10.0ms preprocess, 373.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x800 (no detections), 337.0ms\n",
      "Speed: 7.3ms preprocess, 337.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 341.8ms\n",
      "Speed: 8.4ms preprocess, 341.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 340.9ms\n",
      "Speed: 5.0ms preprocess, 340.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 357.8ms\n",
      "Speed: 6.0ms preprocess, 357.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 335.4ms\n",
      "Speed: 9.8ms preprocess, 335.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 344.0ms\n",
      "Speed: 5.1ms preprocess, 344.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 347.7ms\n",
      "Speed: 6.6ms preprocess, 347.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 335.6ms\n",
      "Speed: 10.1ms preprocess, 335.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 340.6ms\n",
      "Speed: 9.8ms preprocess, 340.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 335.7ms\n",
      "Speed: 11.0ms preprocess, 335.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 354.1ms\n",
      "Speed: 10.0ms preprocess, 354.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 331.8ms\n",
      "Speed: 10.8ms preprocess, 331.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 336.1ms\n",
      "Speed: 8.8ms preprocess, 336.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 330.4ms\n",
      "Speed: 9.0ms preprocess, 330.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 339.5ms\n",
      "Speed: 8.5ms preprocess, 339.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 350.0ms\n",
      "Speed: 2.0ms preprocess, 350.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 340.7ms\n",
      "Speed: 4.0ms preprocess, 340.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 343.0ms\n",
      "Speed: 6.7ms preprocess, 343.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 335.7ms\n",
      "Speed: 8.9ms preprocess, 335.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 338.2ms\n",
      "Speed: 11.0ms preprocess, 338.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 347.1ms\n",
      "Speed: 6.2ms preprocess, 347.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 330.2ms\n",
      "Speed: 13.8ms preprocess, 330.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 342.9ms\n",
      "Speed: 10.5ms preprocess, 342.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 329.9ms\n",
      "Speed: 8.1ms preprocess, 329.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 337.4ms\n",
      "Speed: 11.3ms preprocess, 337.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 322.3ms\n",
      "Speed: 10.0ms preprocess, 322.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 307.8ms\n",
      "Speed: 5.4ms preprocess, 307.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 333.4ms\n",
      "Speed: 7.4ms preprocess, 333.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 333.6ms\n",
      "Speed: 9.8ms preprocess, 333.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 341.4ms\n",
      "Speed: 11.7ms preprocess, 341.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 340.2ms\n",
      "Speed: 6.0ms preprocess, 340.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 354.7ms\n",
      "Speed: 3.9ms preprocess, 354.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 327.3ms\n",
      "Speed: 9.3ms preprocess, 327.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 338.3ms\n",
      "Speed: 11.9ms preprocess, 338.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 346.3ms\n",
      "Speed: 11.5ms preprocess, 346.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 344.0ms\n",
      "Speed: 12.0ms preprocess, 344.0ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 345.4ms\n",
      "Speed: 7.0ms preprocess, 345.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 354.3ms\n",
      "Speed: 8.9ms preprocess, 354.3ms inference, 15.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 344.2ms\n",
      "Speed: 8.3ms preprocess, 344.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 359.7ms\n",
      "Speed: 5.0ms preprocess, 359.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 345.1ms\n",
      "Speed: 9.6ms preprocess, 345.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 443.4ms\n",
      "Speed: 19.0ms preprocess, 443.4ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 385.9ms\n",
      "Speed: 4.3ms preprocess, 385.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 380.2ms\n",
      "Speed: 14.0ms preprocess, 380.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 370.3ms\n",
      "Speed: 5.3ms preprocess, 370.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 346.3ms\n",
      "Speed: 7.1ms preprocess, 346.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 641.4ms\n",
      "Speed: 9.5ms preprocess, 641.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 365.8ms\n",
      "Speed: 5.2ms preprocess, 365.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 381.5ms\n",
      "Speed: 7.7ms preprocess, 381.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import math \n",
    "# start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 1280)\n",
    "cap.set(4, 720)\n",
    "\n",
    "# model\n",
    "model = YOLO(\"yolo-Weights/yolov8n.pt\")\n",
    "\n",
    "# object classes\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "              \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "              ]\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    results = model(img, stream=True)\n",
    "\n",
    "    # coordinates\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "\n",
    "        for box in boxes:\n",
    "            # bounding box\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "\n",
    "            # put box in cam\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "\n",
    "            # confidence\n",
    "            confidence = math.ceil((box.conf[0]*100))/100\n",
    "            print(\"Confidence --->\",confidence)\n",
    "\n",
    "            # class name\n",
    "            cls = int(box.cls[0])\n",
    "            print(\"Class name -->\", classNames[cls])\n",
    "\n",
    "            # object details\n",
    "            org = [x1, y1]\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            fontScale = 1\n",
    "            color = (255, 0, 0)\n",
    "            thickness = 2\n",
    "\n",
    "            cv2.putText(img, classNames[cls], org, font, fontScale, color, thickness)\n",
    "\n",
    "    cv2.imshow('Webcam', img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ce197-fa7d-4a2c-9d08-d14a9c152732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc16452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15afc618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rutvi\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rutvi\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to C:\\Users\\rutvi/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|| 160M/160M [00:32<00:00, 5.21MB/s]\n",
      "C:\\Users\\rutvi\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\rutvi/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|| 97.8M/97.8M [00:18<00:00, 5.64MB/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "FastRCNNPredictor.__init__() got an unexpected keyword argument 'in_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load your new detection head weights\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model_new \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mdetection\u001b[38;5;241m.\u001b[39mfasterrcnn_resnet50_fpn(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 13\u001b[0m model_new\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mdetection\u001b[38;5;241m.\u001b[39mfaster_rcnn\u001b[38;5;241m.\u001b[39mFastRCNNPredictor(\n\u001b[0;32m     14\u001b[0m     in_features\u001b[38;5;241m=\u001b[39mmodel_new\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor\u001b[38;5;241m.\u001b[39mcls_score\u001b[38;5;241m.\u001b[39min_features,\n\u001b[0;32m     15\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39mnum_classes_new\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m model_new\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create a new model with two detection heads\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: FastRCNNPredictor.__init__() got an unexpected keyword argument 'in_features'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_original = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Modify the number of classes in the detection head\n",
    "num_classes_original = model_original.roi_heads.box_predictor.cls_score.out_features\n",
    "num_classes_new = 85  # Total number of classes you want to detect\n",
    "\n",
    "# Load your new detection head weights\n",
    "model_new = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "model_new.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features=model_new.roi_heads.box_predictor.cls_score.in_features,\n",
    "    num_classes=num_classes_new\n",
    ")\n",
    "model_new.load_state_dict(torch.load(\"best.pt\"))\n",
    "\n",
    "# Create a new model with two detection heads\n",
    "model_combined = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "model_combined.backbone = model_original.backbone\n",
    "model_combined.rpn = model_original.rpn\n",
    "model_combined.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features=model_combined.roi_heads.box_predictor.cls_score.in_features,\n",
    "    num_classes=num_classes_original + num_classes_new\n",
    ")\n",
    "\n",
    "# Copy the weights from the original model to the combined model\n",
    "model_combined.backbone.load_state_dict(model_original.backbone.state_dict())\n",
    "model_combined.rpn.load_state_dict(model_original.rpn.state_dict())\n",
    "model_combined.roi_heads.box_predictor.load_state_dict(model_original.roi_heads.box_predictor.state_dict())\n",
    "\n",
    "# Copy the weights from your new model to the combined model\n",
    "model_combined.roi_heads.box_predictor.load_state_dict(model_new.roi_heads.box_predictor.state_dict())\n",
    "\n",
    "# Save the combined model\n",
    "torch.save(model_combined.state_dict(), \"combined_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "822e2032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING  'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\n",
      "WARNING  'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.\n",
      "Note this warning may be related to loading older models. You can update your model to current structure with:\n",
      "    import torch\n",
      "    ckpt = torch.load(\"model.pt\")  # applies to both official and custom models\n",
      "    torch.save(ckpt, \"updated-model.pt\")\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.body.conv1.weight\", \"backbone.body.bn1.weight\", \"backbone.body.bn1.bias\", \"backbone.body.bn1.running_mean\", \"backbone.body.bn1.running_var\", \"backbone.body.layer1.0.conv1.weight\", \"backbone.body.layer1.0.bn1.weight\", \"backbone.body.layer1.0.bn1.bias\", \"backbone.body.layer1.0.bn1.running_mean\", \"backbone.body.layer1.0.bn1.running_var\", \"backbone.body.layer1.0.conv2.weight\", \"backbone.body.layer1.0.bn2.weight\", \"backbone.body.layer1.0.bn2.bias\", \"backbone.body.layer1.0.bn2.running_mean\", \"backbone.body.layer1.0.bn2.running_var\", \"backbone.body.layer1.0.conv3.weight\", \"backbone.body.layer1.0.bn3.weight\", \"backbone.body.layer1.0.bn3.bias\", \"backbone.body.layer1.0.bn3.running_mean\", \"backbone.body.layer1.0.bn3.running_var\", \"backbone.body.layer1.0.downsample.0.weight\", \"backbone.body.layer1.0.downsample.1.weight\", \"backbone.body.layer1.0.downsample.1.bias\", \"backbone.body.layer1.0.downsample.1.running_mean\", \"backbone.body.layer1.0.downsample.1.running_var\", \"backbone.body.layer1.1.conv1.weight\", \"backbone.body.layer1.1.bn1.weight\", \"backbone.body.layer1.1.bn1.bias\", \"backbone.body.layer1.1.bn1.running_mean\", \"backbone.body.layer1.1.bn1.running_var\", \"backbone.body.layer1.1.conv2.weight\", \"backbone.body.layer1.1.bn2.weight\", \"backbone.body.layer1.1.bn2.bias\", \"backbone.body.layer1.1.bn2.running_mean\", \"backbone.body.layer1.1.bn2.running_var\", \"backbone.body.layer1.1.conv3.weight\", \"backbone.body.layer1.1.bn3.weight\", \"backbone.body.layer1.1.bn3.bias\", \"backbone.body.layer1.1.bn3.running_mean\", \"backbone.body.layer1.1.bn3.running_var\", \"backbone.body.layer1.2.conv1.weight\", \"backbone.body.layer1.2.bn1.weight\", \"backbone.body.layer1.2.bn1.bias\", \"backbone.body.layer1.2.bn1.running_mean\", \"backbone.body.layer1.2.bn1.running_var\", \"backbone.body.layer1.2.conv2.weight\", \"backbone.body.layer1.2.bn2.weight\", \"backbone.body.layer1.2.bn2.bias\", \"backbone.body.layer1.2.bn2.running_mean\", \"backbone.body.layer1.2.bn2.running_var\", \"backbone.body.layer1.2.conv3.weight\", \"backbone.body.layer1.2.bn3.weight\", \"backbone.body.layer1.2.bn3.bias\", \"backbone.body.layer1.2.bn3.running_mean\", \"backbone.body.layer1.2.bn3.running_var\", \"backbone.body.layer2.0.conv1.weight\", \"backbone.body.layer2.0.bn1.weight\", \"backbone.body.layer2.0.bn1.bias\", \"backbone.body.layer2.0.bn1.running_mean\", \"backbone.body.layer2.0.bn1.running_var\", \"backbone.body.layer2.0.conv2.weight\", \"backbone.body.layer2.0.bn2.weight\", \"backbone.body.layer2.0.bn2.bias\", \"backbone.body.layer2.0.bn2.running_mean\", \"backbone.body.layer2.0.bn2.running_var\", \"backbone.body.layer2.0.conv3.weight\", \"backbone.body.layer2.0.bn3.weight\", \"backbone.body.layer2.0.bn3.bias\", \"backbone.body.layer2.0.bn3.running_mean\", \"backbone.body.layer2.0.bn3.running_var\", \"backbone.body.layer2.0.downsample.0.weight\", \"backbone.body.layer2.0.downsample.1.weight\", \"backbone.body.layer2.0.downsample.1.bias\", \"backbone.body.layer2.0.downsample.1.running_mean\", \"backbone.body.layer2.0.downsample.1.running_var\", \"backbone.body.layer2.1.conv1.weight\", \"backbone.body.layer2.1.bn1.weight\", \"backbone.body.layer2.1.bn1.bias\", \"backbone.body.layer2.1.bn1.running_mean\", \"backbone.body.layer2.1.bn1.running_var\", \"backbone.body.layer2.1.conv2.weight\", \"backbone.body.layer2.1.bn2.weight\", \"backbone.body.layer2.1.bn2.bias\", \"backbone.body.layer2.1.bn2.running_mean\", \"backbone.body.layer2.1.bn2.running_var\", \"backbone.body.layer2.1.conv3.weight\", \"backbone.body.layer2.1.bn3.weight\", \"backbone.body.layer2.1.bn3.bias\", \"backbone.body.layer2.1.bn3.running_mean\", \"backbone.body.layer2.1.bn3.running_var\", \"backbone.body.layer2.2.conv1.weight\", \"backbone.body.layer2.2.bn1.weight\", \"backbone.body.layer2.2.bn1.bias\", \"backbone.body.layer2.2.bn1.running_mean\", \"backbone.body.layer2.2.bn1.running_var\", \"backbone.body.layer2.2.conv2.weight\", \"backbone.body.layer2.2.bn2.weight\", \"backbone.body.layer2.2.bn2.bias\", \"backbone.body.layer2.2.bn2.running_mean\", \"backbone.body.layer2.2.bn2.running_var\", \"backbone.body.layer2.2.conv3.weight\", \"backbone.body.layer2.2.bn3.weight\", \"backbone.body.layer2.2.bn3.bias\", \"backbone.body.layer2.2.bn3.running_mean\", \"backbone.body.layer2.2.bn3.running_var\", \"backbone.body.layer2.3.conv1.weight\", \"backbone.body.layer2.3.bn1.weight\", \"backbone.body.layer2.3.bn1.bias\", \"backbone.body.layer2.3.bn1.running_mean\", \"backbone.body.layer2.3.bn1.running_var\", \"backbone.body.layer2.3.conv2.weight\", \"backbone.body.layer2.3.bn2.weight\", \"backbone.body.layer2.3.bn2.bias\", \"backbone.body.layer2.3.bn2.running_mean\", \"backbone.body.layer2.3.bn2.running_var\", \"backbone.body.layer2.3.conv3.weight\", \"backbone.body.layer2.3.bn3.weight\", \"backbone.body.layer2.3.bn3.bias\", \"backbone.body.layer2.3.bn3.running_mean\", \"backbone.body.layer2.3.bn3.running_var\", \"backbone.body.layer3.0.conv1.weight\", \"backbone.body.layer3.0.bn1.weight\", \"backbone.body.layer3.0.bn1.bias\", \"backbone.body.layer3.0.bn1.running_mean\", \"backbone.body.layer3.0.bn1.running_var\", \"backbone.body.layer3.0.conv2.weight\", \"backbone.body.layer3.0.bn2.weight\", \"backbone.body.layer3.0.bn2.bias\", \"backbone.body.layer3.0.bn2.running_mean\", \"backbone.body.layer3.0.bn2.running_var\", \"backbone.body.layer3.0.conv3.weight\", \"backbone.body.layer3.0.bn3.weight\", \"backbone.body.layer3.0.bn3.bias\", \"backbone.body.layer3.0.bn3.running_mean\", \"backbone.body.layer3.0.bn3.running_var\", \"backbone.body.layer3.0.downsample.0.weight\", \"backbone.body.layer3.0.downsample.1.weight\", \"backbone.body.layer3.0.downsample.1.bias\", \"backbone.body.layer3.0.downsample.1.running_mean\", \"backbone.body.layer3.0.downsample.1.running_var\", \"backbone.body.layer3.1.conv1.weight\", \"backbone.body.layer3.1.bn1.weight\", \"backbone.body.layer3.1.bn1.bias\", \"backbone.body.layer3.1.bn1.running_mean\", \"backbone.body.layer3.1.bn1.running_var\", \"backbone.body.layer3.1.conv2.weight\", \"backbone.body.layer3.1.bn2.weight\", \"backbone.body.layer3.1.bn2.bias\", \"backbone.body.layer3.1.bn2.running_mean\", \"backbone.body.layer3.1.bn2.running_var\", \"backbone.body.layer3.1.conv3.weight\", \"backbone.body.layer3.1.bn3.weight\", \"backbone.body.layer3.1.bn3.bias\", \"backbone.body.layer3.1.bn3.running_mean\", \"backbone.body.layer3.1.bn3.running_var\", \"backbone.body.layer3.2.conv1.weight\", \"backbone.body.layer3.2.bn1.weight\", \"backbone.body.layer3.2.bn1.bias\", \"backbone.body.layer3.2.bn1.running_mean\", \"backbone.body.layer3.2.bn1.running_var\", \"backbone.body.layer3.2.conv2.weight\", \"backbone.body.layer3.2.bn2.weight\", \"backbone.body.layer3.2.bn2.bias\", \"backbone.body.layer3.2.bn2.running_mean\", \"backbone.body.layer3.2.bn2.running_var\", \"backbone.body.layer3.2.conv3.weight\", \"backbone.body.layer3.2.bn3.weight\", \"backbone.body.layer3.2.bn3.bias\", \"backbone.body.layer3.2.bn3.running_mean\", \"backbone.body.layer3.2.bn3.running_var\", \"backbone.body.layer3.3.conv1.weight\", \"backbone.body.layer3.3.bn1.weight\", \"backbone.body.layer3.3.bn1.bias\", \"backbone.body.layer3.3.bn1.running_mean\", \"backbone.body.layer3.3.bn1.running_var\", \"backbone.body.layer3.3.conv2.weight\", \"backbone.body.layer3.3.bn2.weight\", \"backbone.body.layer3.3.bn2.bias\", \"backbone.body.layer3.3.bn2.running_mean\", \"backbone.body.layer3.3.bn2.running_var\", \"backbone.body.layer3.3.conv3.weight\", \"backbone.body.layer3.3.bn3.weight\", \"backbone.body.layer3.3.bn3.bias\", \"backbone.body.layer3.3.bn3.running_mean\", \"backbone.body.layer3.3.bn3.running_var\", \"backbone.body.layer3.4.conv1.weight\", \"backbone.body.layer3.4.bn1.weight\", \"backbone.body.layer3.4.bn1.bias\", \"backbone.body.layer3.4.bn1.running_mean\", \"backbone.body.layer3.4.bn1.running_var\", \"backbone.body.layer3.4.conv2.weight\", \"backbone.body.layer3.4.bn2.weight\", \"backbone.body.layer3.4.bn2.bias\", \"backbone.body.layer3.4.bn2.running_mean\", \"backbone.body.layer3.4.bn2.running_var\", \"backbone.body.layer3.4.conv3.weight\", \"backbone.body.layer3.4.bn3.weight\", \"backbone.body.layer3.4.bn3.bias\", \"backbone.body.layer3.4.bn3.running_mean\", \"backbone.body.layer3.4.bn3.running_var\", \"backbone.body.layer3.5.conv1.weight\", \"backbone.body.layer3.5.bn1.weight\", \"backbone.body.layer3.5.bn1.bias\", \"backbone.body.layer3.5.bn1.running_mean\", \"backbone.body.layer3.5.bn1.running_var\", \"backbone.body.layer3.5.conv2.weight\", \"backbone.body.layer3.5.bn2.weight\", \"backbone.body.layer3.5.bn2.bias\", \"backbone.body.layer3.5.bn2.running_mean\", \"backbone.body.layer3.5.bn2.running_var\", \"backbone.body.layer3.5.conv3.weight\", \"backbone.body.layer3.5.bn3.weight\", \"backbone.body.layer3.5.bn3.bias\", \"backbone.body.layer3.5.bn3.running_mean\", \"backbone.body.layer3.5.bn3.running_var\", \"backbone.body.layer4.0.conv1.weight\", \"backbone.body.layer4.0.bn1.weight\", \"backbone.body.layer4.0.bn1.bias\", \"backbone.body.layer4.0.bn1.running_mean\", \"backbone.body.layer4.0.bn1.running_var\", \"backbone.body.layer4.0.conv2.weight\", \"backbone.body.layer4.0.bn2.weight\", \"backbone.body.layer4.0.bn2.bias\", \"backbone.body.layer4.0.bn2.running_mean\", \"backbone.body.layer4.0.bn2.running_var\", \"backbone.body.layer4.0.conv3.weight\", \"backbone.body.layer4.0.bn3.weight\", \"backbone.body.layer4.0.bn3.bias\", \"backbone.body.layer4.0.bn3.running_mean\", \"backbone.body.layer4.0.bn3.running_var\", \"backbone.body.layer4.0.downsample.0.weight\", \"backbone.body.layer4.0.downsample.1.weight\", \"backbone.body.layer4.0.downsample.1.bias\", \"backbone.body.layer4.0.downsample.1.running_mean\", \"backbone.body.layer4.0.downsample.1.running_var\", \"backbone.body.layer4.1.conv1.weight\", \"backbone.body.layer4.1.bn1.weight\", \"backbone.body.layer4.1.bn1.bias\", \"backbone.body.layer4.1.bn1.running_mean\", \"backbone.body.layer4.1.bn1.running_var\", \"backbone.body.layer4.1.conv2.weight\", \"backbone.body.layer4.1.bn2.weight\", \"backbone.body.layer4.1.bn2.bias\", \"backbone.body.layer4.1.bn2.running_mean\", \"backbone.body.layer4.1.bn2.running_var\", \"backbone.body.layer4.1.conv3.weight\", \"backbone.body.layer4.1.bn3.weight\", \"backbone.body.layer4.1.bn3.bias\", \"backbone.body.layer4.1.bn3.running_mean\", \"backbone.body.layer4.1.bn3.running_var\", \"backbone.body.layer4.2.conv1.weight\", \"backbone.body.layer4.2.bn1.weight\", \"backbone.body.layer4.2.bn1.bias\", \"backbone.body.layer4.2.bn1.running_mean\", \"backbone.body.layer4.2.bn1.running_var\", \"backbone.body.layer4.2.conv2.weight\", \"backbone.body.layer4.2.bn2.weight\", \"backbone.body.layer4.2.bn2.bias\", \"backbone.body.layer4.2.bn2.running_mean\", \"backbone.body.layer4.2.bn2.running_var\", \"backbone.body.layer4.2.conv3.weight\", \"backbone.body.layer4.2.bn3.weight\", \"backbone.body.layer4.2.bn3.bias\", \"backbone.body.layer4.2.bn3.running_mean\", \"backbone.body.layer4.2.bn3.running_var\", \"backbone.fpn.inner_blocks.0.0.weight\", \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.weight\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.weight\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.weight\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.weight\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.weight\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.weight\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.weight\", \"backbone.fpn.layer_blocks.3.0.bias\", \"rpn.head.conv.0.0.weight\", \"rpn.head.conv.0.0.bias\", \"rpn.head.cls_logits.weight\", \"rpn.head.cls_logits.bias\", \"rpn.head.bbox_pred.weight\", \"rpn.head.bbox_pred.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\", \"roi_heads.box_predictor.cls_score.weight\", \"roi_heads.box_predictor.cls_score.bias\", \"roi_heads.box_predictor.bbox_pred.weight\", \"roi_heads.box_predictor.bbox_pred.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"best_fitness\", \"model\", \"ema\", \"updates\", \"optimizer\", \"train_args\", \"date\", \"version\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m model_new \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mdetection\u001b[38;5;241m.\u001b[39mfasterrcnn_resnet50_fpn(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m model_new\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mdetection\u001b[38;5;241m.\u001b[39mfaster_rcnn\u001b[38;5;241m.\u001b[39mFastRCNNPredictor(\n\u001b[0;32m     14\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39mmodel_new\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor\u001b[38;5;241m.\u001b[39mcls_score\u001b[38;5;241m.\u001b[39min_features,\n\u001b[0;32m     15\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39mnum_classes_new\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m model_new\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create a new model with two detection heads\u001b[39;00m\n\u001b[0;32m     20\u001b[0m model_combined \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mdetection\u001b[38;5;241m.\u001b[39mfasterrcnn_resnet50_fpn(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.body.conv1.weight\", \"backbone.body.bn1.weight\", \"backbone.body.bn1.bias\", \"backbone.body.bn1.running_mean\", \"backbone.body.bn1.running_var\", \"backbone.body.layer1.0.conv1.weight\", \"backbone.body.layer1.0.bn1.weight\", \"backbone.body.layer1.0.bn1.bias\", \"backbone.body.layer1.0.bn1.running_mean\", \"backbone.body.layer1.0.bn1.running_var\", \"backbone.body.layer1.0.conv2.weight\", \"backbone.body.layer1.0.bn2.weight\", \"backbone.body.layer1.0.bn2.bias\", \"backbone.body.layer1.0.bn2.running_mean\", \"backbone.body.layer1.0.bn2.running_var\", \"backbone.body.layer1.0.conv3.weight\", \"backbone.body.layer1.0.bn3.weight\", \"backbone.body.layer1.0.bn3.bias\", \"backbone.body.layer1.0.bn3.running_mean\", \"backbone.body.layer1.0.bn3.running_var\", \"backbone.body.layer1.0.downsample.0.weight\", \"backbone.body.layer1.0.downsample.1.weight\", \"backbone.body.layer1.0.downsample.1.bias\", \"backbone.body.layer1.0.downsample.1.running_mean\", \"backbone.body.layer1.0.downsample.1.running_var\", \"backbone.body.layer1.1.conv1.weight\", \"backbone.body.layer1.1.bn1.weight\", \"backbone.body.layer1.1.bn1.bias\", \"backbone.body.layer1.1.bn1.running_mean\", \"backbone.body.layer1.1.bn1.running_var\", \"backbone.body.layer1.1.conv2.weight\", \"backbone.body.layer1.1.bn2.weight\", \"backbone.body.layer1.1.bn2.bias\", \"backbone.body.layer1.1.bn2.running_mean\", \"backbone.body.layer1.1.bn2.running_var\", \"backbone.body.layer1.1.conv3.weight\", \"backbone.body.layer1.1.bn3.weight\", \"backbone.body.layer1.1.bn3.bias\", \"backbone.body.layer1.1.bn3.running_mean\", \"backbone.body.layer1.1.bn3.running_var\", \"backbone.body.layer1.2.conv1.weight\", \"backbone.body.layer1.2.bn1.weight\", \"backbone.body.layer1.2.bn1.bias\", \"backbone.body.layer1.2.bn1.running_mean\", \"backbone.body.layer1.2.bn1.running_var\", \"backbone.body.layer1.2.conv2.weight\", \"backbone.body.layer1.2.bn2.weight\", \"backbone.body.layer1.2.bn2.bias\", \"backbone.body.layer1.2.bn2.running_mean\", \"backbone.body.layer1.2.bn2.running_var\", \"backbone.body.layer1.2.conv3.weight\", \"backbone.body.layer1.2.bn3.weight\", \"backbone.body.layer1.2.bn3.bias\", \"backbone.body.layer1.2.bn3.running_mean\", \"backbone.body.layer1.2.bn3.running_var\", \"backbone.body.layer2.0.conv1.weight\", \"backbone.body.layer2.0.bn1.weight\", \"backbone.body.layer2.0.bn1.bias\", \"backbone.body.layer2.0.bn1.running_mean\", \"backbone.body.layer2.0.bn1.running_var\", \"backbone.body.layer2.0.conv2.weight\", \"backbone.body.layer2.0.bn2.weight\", \"backbone.body.layer2.0.bn2.bias\", \"backbone.body.layer2.0.bn2.running_mean\", \"backbone.body.layer2.0.bn2.running_var\", \"backbone.body.layer2.0.conv3.weight\", \"backbone.body.layer2.0.bn3.weight\", \"backbone.body.layer2.0.bn3.bias\", \"backbone.body.layer2.0.bn3.running_mean\", \"backbone.body.layer2.0.bn3.running_var\", \"backbone.body.layer2.0.downsample.0.weight\", \"backbone.body.layer2.0.downsample.1.weight\", \"backbone.body.layer2.0.downsample.1.bias\", \"backbone.body.layer2.0.downsample.1.running_mean\", \"backbone.body.layer2.0.downsample.1.running_var\", \"backbone.body.layer2.1.conv1.weight\", \"backbone.body.layer2.1.bn1.weight\", \"backbone.body.layer2.1.bn1.bias\", \"backbone.body.layer2.1.bn1.running_mean\", \"backbone.body.layer2.1.bn1.running_var\", \"backbone.body.layer2.1.conv2.weight\", \"backbone.body.layer2.1.bn2.weight\", \"backbone.body.layer2.1.bn2.bias\", \"backbone.body.layer2.1.bn2.running_mean\", \"backbone.body.layer2.1.bn2.running_var\", \"backbone.body.layer2.1.conv3.weight\", \"backbone.body.layer2.1.bn3.weight\", \"backbone.body.layer2.1.bn3.bias\", \"backbone.body.layer2.1.bn3.running_mean\", \"backbone.body.layer2.1.bn3.running_var\", \"backbone.body.layer2.2.conv1.weight\", \"backbone.body.layer2.2.bn1.weight\", \"backbone.body.layer2.2.bn1.bias\", \"backbone.body.layer2.2.bn1.running_mean\", \"backbone.body.layer2.2.bn1.running_var\", \"backbone.body.layer2.2.conv2.weight\", \"backbone.body.layer2.2.bn2.weight\", \"backbone.body.layer2.2.bn2.bias\", \"backbone.body.layer2.2.bn2.running_mean\", \"backbone.body.layer2.2.bn2.running_var\", \"backbone.body.layer2.2.conv3.weight\", \"backbone.body.layer2.2.bn3.weight\", \"backbone.body.layer2.2.bn3.bias\", \"backbone.body.layer2.2.bn3.running_mean\", \"backbone.body.layer2.2.bn3.running_var\", \"backbone.body.layer2.3.conv1.weight\", \"backbone.body.layer2.3.bn1.weight\", \"backbone.body.layer2.3.bn1.bias\", \"backbone.body.layer2.3.bn1.running_mean\", \"backbone.body.layer2.3.bn1.running_var\", \"backbone.body.layer2.3.conv2.weight\", \"backbone.body.layer2.3.bn2.weight\", \"backbone.body.layer2.3.bn2.bias\", \"backbone.body.layer2.3.bn2.running_mean\", \"backbone.body.layer2.3.bn2.running_var\", \"backbone.body.layer2.3.conv3.weight\", \"backbone.body.layer2.3.bn3.weight\", \"backbone.body.layer2.3.bn3.bias\", \"backbone.body.layer2.3.bn3.running_mean\", \"backbone.body.layer2.3.bn3.running_var\", \"backbone.body.layer3.0.conv1.weight\", \"backbone.body.layer3.0.bn1.weight\", \"backbone.body.layer3.0.bn1.bias\", \"backbone.body.layer3.0.bn1.running_mean\", \"backbone.body.layer3.0.bn1.running_var\", \"backbone.body.layer3.0.conv2.weight\", \"backbone.body.layer3.0.bn2.weight\", \"backbone.body.layer3.0.bn2.bias\", \"backbone.body.layer3.0.bn2.running_mean\", \"backbone.body.layer3.0.bn2.running_var\", \"backbone.body.layer3.0.conv3.weight\", \"backbone.body.layer3.0.bn3.weight\", \"backbone.body.layer3.0.bn3.bias\", \"backbone.body.layer3.0.bn3.running_mean\", \"backbone.body.layer3.0.bn3.running_var\", \"backbone.body.layer3.0.downsample.0.weight\", \"backbone.body.layer3.0.downsample.1.weight\", \"backbone.body.layer3.0.downsample.1.bias\", \"backbone.body.layer3.0.downsample.1.running_mean\", \"backbone.body.layer3.0.downsample.1.running_var\", \"backbone.body.layer3.1.conv1.weight\", \"backbone.body.layer3.1.bn1.weight\", \"backbone.body.layer3.1.bn1.bias\", \"backbone.body.layer3.1.bn1.running_mean\", \"backbone.body.layer3.1.bn1.running_var\", \"backbone.body.layer3.1.conv2.weight\", \"backbone.body.layer3.1.bn2.weight\", \"backbone.body.layer3.1.bn2.bias\", \"backbone.body.layer3.1.bn2.running_mean\", \"backbone.body.layer3.1.bn2.running_var\", \"backbone.body.layer3.1.conv3.weight\", \"backbone.body.layer3.1.bn3.weight\", \"backbone.body.layer3.1.bn3.bias\", \"backbone.body.layer3.1.bn3.running_mean\", \"backbone.body.layer3.1.bn3.running_var\", \"backbone.body.layer3.2.conv1.weight\", \"backbone.body.layer3.2.bn1.weight\", \"backbone.body.layer3.2.bn1.bias\", \"backbone.body.layer3.2.bn1.running_mean\", \"backbone.body.layer3.2.bn1.running_var\", \"backbone.body.layer3.2.conv2.weight\", \"backbone.body.layer3.2.bn2.weight\", \"backbone.body.layer3.2.bn2.bias\", \"backbone.body.layer3.2.bn2.running_mean\", \"backbone.body.layer3.2.bn2.running_var\", \"backbone.body.layer3.2.conv3.weight\", \"backbone.body.layer3.2.bn3.weight\", \"backbone.body.layer3.2.bn3.bias\", \"backbone.body.layer3.2.bn3.running_mean\", \"backbone.body.layer3.2.bn3.running_var\", \"backbone.body.layer3.3.conv1.weight\", \"backbone.body.layer3.3.bn1.weight\", \"backbone.body.layer3.3.bn1.bias\", \"backbone.body.layer3.3.bn1.running_mean\", \"backbone.body.layer3.3.bn1.running_var\", \"backbone.body.layer3.3.conv2.weight\", \"backbone.body.layer3.3.bn2.weight\", \"backbone.body.layer3.3.bn2.bias\", \"backbone.body.layer3.3.bn2.running_mean\", \"backbone.body.layer3.3.bn2.running_var\", \"backbone.body.layer3.3.conv3.weight\", \"backbone.body.layer3.3.bn3.weight\", \"backbone.body.layer3.3.bn3.bias\", \"backbone.body.layer3.3.bn3.running_mean\", \"backbone.body.layer3.3.bn3.running_var\", \"backbone.body.layer3.4.conv1.weight\", \"backbone.body.layer3.4.bn1.weight\", \"backbone.body.layer3.4.bn1.bias\", \"backbone.body.layer3.4.bn1.running_mean\", \"backbone.body.layer3.4.bn1.running_var\", \"backbone.body.layer3.4.conv2.weight\", \"backbone.body.layer3.4.bn2.weight\", \"backbone.body.layer3.4.bn2.bias\", \"backbone.body.layer3.4.bn2.running_mean\", \"backbone.body.layer3.4.bn2.running_var\", \"backbone.body.layer3.4.conv3.weight\", \"backbone.body.layer3.4.bn3.weight\", \"backbone.body.layer3.4.bn3.bias\", \"backbone.body.layer3.4.bn3.running_mean\", \"backbone.body.layer3.4.bn3.running_var\", \"backbone.body.layer3.5.conv1.weight\", \"backbone.body.layer3.5.bn1.weight\", \"backbone.body.layer3.5.bn1.bias\", \"backbone.body.layer3.5.bn1.running_mean\", \"backbone.body.layer3.5.bn1.running_var\", \"backbone.body.layer3.5.conv2.weight\", \"backbone.body.layer3.5.bn2.weight\", \"backbone.body.layer3.5.bn2.bias\", \"backbone.body.layer3.5.bn2.running_mean\", \"backbone.body.layer3.5.bn2.running_var\", \"backbone.body.layer3.5.conv3.weight\", \"backbone.body.layer3.5.bn3.weight\", \"backbone.body.layer3.5.bn3.bias\", \"backbone.body.layer3.5.bn3.running_mean\", \"backbone.body.layer3.5.bn3.running_var\", \"backbone.body.layer4.0.conv1.weight\", \"backbone.body.layer4.0.bn1.weight\", \"backbone.body.layer4.0.bn1.bias\", \"backbone.body.layer4.0.bn1.running_mean\", \"backbone.body.layer4.0.bn1.running_var\", \"backbone.body.layer4.0.conv2.weight\", \"backbone.body.layer4.0.bn2.weight\", \"backbone.body.layer4.0.bn2.bias\", \"backbone.body.layer4.0.bn2.running_mean\", \"backbone.body.layer4.0.bn2.running_var\", \"backbone.body.layer4.0.conv3.weight\", \"backbone.body.layer4.0.bn3.weight\", \"backbone.body.layer4.0.bn3.bias\", \"backbone.body.layer4.0.bn3.running_mean\", \"backbone.body.layer4.0.bn3.running_var\", \"backbone.body.layer4.0.downsample.0.weight\", \"backbone.body.layer4.0.downsample.1.weight\", \"backbone.body.layer4.0.downsample.1.bias\", \"backbone.body.layer4.0.downsample.1.running_mean\", \"backbone.body.layer4.0.downsample.1.running_var\", \"backbone.body.layer4.1.conv1.weight\", \"backbone.body.layer4.1.bn1.weight\", \"backbone.body.layer4.1.bn1.bias\", \"backbone.body.layer4.1.bn1.running_mean\", \"backbone.body.layer4.1.bn1.running_var\", \"backbone.body.layer4.1.conv2.weight\", \"backbone.body.layer4.1.bn2.weight\", \"backbone.body.layer4.1.bn2.bias\", \"backbone.body.layer4.1.bn2.running_mean\", \"backbone.body.layer4.1.bn2.running_var\", \"backbone.body.layer4.1.conv3.weight\", \"backbone.body.layer4.1.bn3.weight\", \"backbone.body.layer4.1.bn3.bias\", \"backbone.body.layer4.1.bn3.running_mean\", \"backbone.body.layer4.1.bn3.running_var\", \"backbone.body.layer4.2.conv1.weight\", \"backbone.body.layer4.2.bn1.weight\", \"backbone.body.layer4.2.bn1.bias\", \"backbone.body.layer4.2.bn1.running_mean\", \"backbone.body.layer4.2.bn1.running_var\", \"backbone.body.layer4.2.conv2.weight\", \"backbone.body.layer4.2.bn2.weight\", \"backbone.body.layer4.2.bn2.bias\", \"backbone.body.layer4.2.bn2.running_mean\", \"backbone.body.layer4.2.bn2.running_var\", \"backbone.body.layer4.2.conv3.weight\", \"backbone.body.layer4.2.bn3.weight\", \"backbone.body.layer4.2.bn3.bias\", \"backbone.body.layer4.2.bn3.running_mean\", \"backbone.body.layer4.2.bn3.running_var\", \"backbone.fpn.inner_blocks.0.0.weight\", \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.weight\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.weight\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.weight\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.weight\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.weight\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.weight\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.weight\", \"backbone.fpn.layer_blocks.3.0.bias\", \"rpn.head.conv.0.0.weight\", \"rpn.head.conv.0.0.bias\", \"rpn.head.cls_logits.weight\", \"rpn.head.cls_logits.bias\", \"rpn.head.bbox_pred.weight\", \"rpn.head.bbox_pred.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\", \"roi_heads.box_predictor.cls_score.weight\", \"roi_heads.box_predictor.cls_score.bias\", \"roi_heads.box_predictor.bbox_pred.weight\", \"roi_heads.box_predictor.bbox_pred.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"best_fitness\", \"model\", \"ema\", \"updates\", \"optimizer\", \"train_args\", \"date\", \"version\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_original = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Modify the number of classes in the detection head\n",
    "num_classes_original = model_original.roi_heads.box_predictor.cls_score.out_features\n",
    "num_classes_new = 5  # Number of new classes you want to detect\n",
    "\n",
    "# Load your new detection head weights\n",
    "model_new = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "model_new.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_channels=model_new.roi_heads.box_predictor.cls_score.in_features,\n",
    "    num_classes=num_classes_new\n",
    ")\n",
    "model_new.load_state_dict(torch.load(\"best.pt\"))\n",
    "\n",
    "# Create a new model with two detection heads\n",
    "model_combined = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "model_combined.backbone = model_original.backbone\n",
    "model_combined.rpn = model_original.rpn\n",
    "model_combined.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_channels=model_combined.roi_heads.box_predictor.cls_score.in_features,\n",
    "    num_classes=num_classes_original + num_classes_new\n",
    ")\n",
    "\n",
    "# Copy the weights from the original model to the combined model\n",
    "model_combined.backbone.load_state_dict(model_original.backbone.state_dict())\n",
    "model_combined.rpn.load_state_dict(model_original.rpn.state_dict())\n",
    "model_combined.roi_heads.box_predictor.load_state_dict(model_original.roi_heads.box_predictor.state_dict())\n",
    "\n",
    "# Copy the weights from your new model to the combined model\n",
    "model_combined.roi_heads.box_predictor.load_state_dict(model_new.roi_heads.box_predictor.state_dict())\n",
    "\n",
    "# Save the combined model\n",
    "torch.save(model_combined.state_dict(), \"combined_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1adddb38",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.body.conv1.weight\", \"backbone.body.bn1.weight\", \"backbone.body.bn1.bias\", \"backbone.body.bn1.running_mean\", \"backbone.body.bn1.running_var\", \"backbone.body.layer1.0.conv1.weight\", \"backbone.body.layer1.0.bn1.weight\", \"backbone.body.layer1.0.bn1.bias\", \"backbone.body.layer1.0.bn1.running_mean\", \"backbone.body.layer1.0.bn1.running_var\", \"backbone.body.layer1.0.conv2.weight\", \"backbone.body.layer1.0.bn2.weight\", \"backbone.body.layer1.0.bn2.bias\", \"backbone.body.layer1.0.bn2.running_mean\", \"backbone.body.layer1.0.bn2.running_var\", \"backbone.body.layer1.0.conv3.weight\", \"backbone.body.layer1.0.bn3.weight\", \"backbone.body.layer1.0.bn3.bias\", \"backbone.body.layer1.0.bn3.running_mean\", \"backbone.body.layer1.0.bn3.running_var\", \"backbone.body.layer1.0.downsample.0.weight\", \"backbone.body.layer1.0.downsample.1.weight\", \"backbone.body.layer1.0.downsample.1.bias\", \"backbone.body.layer1.0.downsample.1.running_mean\", \"backbone.body.layer1.0.downsample.1.running_var\", \"backbone.body.layer1.1.conv1.weight\", \"backbone.body.layer1.1.bn1.weight\", \"backbone.body.layer1.1.bn1.bias\", \"backbone.body.layer1.1.bn1.running_mean\", \"backbone.body.layer1.1.bn1.running_var\", \"backbone.body.layer1.1.conv2.weight\", \"backbone.body.layer1.1.bn2.weight\", \"backbone.body.layer1.1.bn2.bias\", \"backbone.body.layer1.1.bn2.running_mean\", \"backbone.body.layer1.1.bn2.running_var\", \"backbone.body.layer1.1.conv3.weight\", \"backbone.body.layer1.1.bn3.weight\", \"backbone.body.layer1.1.bn3.bias\", \"backbone.body.layer1.1.bn3.running_mean\", \"backbone.body.layer1.1.bn3.running_var\", \"backbone.body.layer1.2.conv1.weight\", \"backbone.body.layer1.2.bn1.weight\", \"backbone.body.layer1.2.bn1.bias\", \"backbone.body.layer1.2.bn1.running_mean\", \"backbone.body.layer1.2.bn1.running_var\", \"backbone.body.layer1.2.conv2.weight\", \"backbone.body.layer1.2.bn2.weight\", \"backbone.body.layer1.2.bn2.bias\", \"backbone.body.layer1.2.bn2.running_mean\", \"backbone.body.layer1.2.bn2.running_var\", \"backbone.body.layer1.2.conv3.weight\", \"backbone.body.layer1.2.bn3.weight\", \"backbone.body.layer1.2.bn3.bias\", \"backbone.body.layer1.2.bn3.running_mean\", \"backbone.body.layer1.2.bn3.running_var\", \"backbone.body.layer2.0.conv1.weight\", \"backbone.body.layer2.0.bn1.weight\", \"backbone.body.layer2.0.bn1.bias\", \"backbone.body.layer2.0.bn1.running_mean\", \"backbone.body.layer2.0.bn1.running_var\", \"backbone.body.layer2.0.conv2.weight\", \"backbone.body.layer2.0.bn2.weight\", \"backbone.body.layer2.0.bn2.bias\", \"backbone.body.layer2.0.bn2.running_mean\", \"backbone.body.layer2.0.bn2.running_var\", \"backbone.body.layer2.0.conv3.weight\", \"backbone.body.layer2.0.bn3.weight\", \"backbone.body.layer2.0.bn3.bias\", \"backbone.body.layer2.0.bn3.running_mean\", \"backbone.body.layer2.0.bn3.running_var\", \"backbone.body.layer2.0.downsample.0.weight\", \"backbone.body.layer2.0.downsample.1.weight\", \"backbone.body.layer2.0.downsample.1.bias\", \"backbone.body.layer2.0.downsample.1.running_mean\", \"backbone.body.layer2.0.downsample.1.running_var\", \"backbone.body.layer2.1.conv1.weight\", \"backbone.body.layer2.1.bn1.weight\", \"backbone.body.layer2.1.bn1.bias\", \"backbone.body.layer2.1.bn1.running_mean\", \"backbone.body.layer2.1.bn1.running_var\", \"backbone.body.layer2.1.conv2.weight\", \"backbone.body.layer2.1.bn2.weight\", \"backbone.body.layer2.1.bn2.bias\", \"backbone.body.layer2.1.bn2.running_mean\", \"backbone.body.layer2.1.bn2.running_var\", \"backbone.body.layer2.1.conv3.weight\", \"backbone.body.layer2.1.bn3.weight\", \"backbone.body.layer2.1.bn3.bias\", \"backbone.body.layer2.1.bn3.running_mean\", \"backbone.body.layer2.1.bn3.running_var\", \"backbone.body.layer2.2.conv1.weight\", \"backbone.body.layer2.2.bn1.weight\", \"backbone.body.layer2.2.bn1.bias\", \"backbone.body.layer2.2.bn1.running_mean\", \"backbone.body.layer2.2.bn1.running_var\", \"backbone.body.layer2.2.conv2.weight\", \"backbone.body.layer2.2.bn2.weight\", \"backbone.body.layer2.2.bn2.bias\", \"backbone.body.layer2.2.bn2.running_mean\", \"backbone.body.layer2.2.bn2.running_var\", \"backbone.body.layer2.2.conv3.weight\", \"backbone.body.layer2.2.bn3.weight\", \"backbone.body.layer2.2.bn3.bias\", \"backbone.body.layer2.2.bn3.running_mean\", \"backbone.body.layer2.2.bn3.running_var\", \"backbone.body.layer2.3.conv1.weight\", \"backbone.body.layer2.3.bn1.weight\", \"backbone.body.layer2.3.bn1.bias\", \"backbone.body.layer2.3.bn1.running_mean\", \"backbone.body.layer2.3.bn1.running_var\", \"backbone.body.layer2.3.conv2.weight\", \"backbone.body.layer2.3.bn2.weight\", \"backbone.body.layer2.3.bn2.bias\", \"backbone.body.layer2.3.bn2.running_mean\", \"backbone.body.layer2.3.bn2.running_var\", \"backbone.body.layer2.3.conv3.weight\", \"backbone.body.layer2.3.bn3.weight\", \"backbone.body.layer2.3.bn3.bias\", \"backbone.body.layer2.3.bn3.running_mean\", \"backbone.body.layer2.3.bn3.running_var\", \"backbone.body.layer3.0.conv1.weight\", \"backbone.body.layer3.0.bn1.weight\", \"backbone.body.layer3.0.bn1.bias\", \"backbone.body.layer3.0.bn1.running_mean\", \"backbone.body.layer3.0.bn1.running_var\", \"backbone.body.layer3.0.conv2.weight\", \"backbone.body.layer3.0.bn2.weight\", \"backbone.body.layer3.0.bn2.bias\", \"backbone.body.layer3.0.bn2.running_mean\", \"backbone.body.layer3.0.bn2.running_var\", \"backbone.body.layer3.0.conv3.weight\", \"backbone.body.layer3.0.bn3.weight\", \"backbone.body.layer3.0.bn3.bias\", \"backbone.body.layer3.0.bn3.running_mean\", \"backbone.body.layer3.0.bn3.running_var\", \"backbone.body.layer3.0.downsample.0.weight\", \"backbone.body.layer3.0.downsample.1.weight\", \"backbone.body.layer3.0.downsample.1.bias\", \"backbone.body.layer3.0.downsample.1.running_mean\", \"backbone.body.layer3.0.downsample.1.running_var\", \"backbone.body.layer3.1.conv1.weight\", \"backbone.body.layer3.1.bn1.weight\", \"backbone.body.layer3.1.bn1.bias\", \"backbone.body.layer3.1.bn1.running_mean\", \"backbone.body.layer3.1.bn1.running_var\", \"backbone.body.layer3.1.conv2.weight\", \"backbone.body.layer3.1.bn2.weight\", \"backbone.body.layer3.1.bn2.bias\", \"backbone.body.layer3.1.bn2.running_mean\", \"backbone.body.layer3.1.bn2.running_var\", \"backbone.body.layer3.1.conv3.weight\", \"backbone.body.layer3.1.bn3.weight\", \"backbone.body.layer3.1.bn3.bias\", \"backbone.body.layer3.1.bn3.running_mean\", \"backbone.body.layer3.1.bn3.running_var\", \"backbone.body.layer3.2.conv1.weight\", \"backbone.body.layer3.2.bn1.weight\", \"backbone.body.layer3.2.bn1.bias\", \"backbone.body.layer3.2.bn1.running_mean\", \"backbone.body.layer3.2.bn1.running_var\", \"backbone.body.layer3.2.conv2.weight\", \"backbone.body.layer3.2.bn2.weight\", \"backbone.body.layer3.2.bn2.bias\", \"backbone.body.layer3.2.bn2.running_mean\", \"backbone.body.layer3.2.bn2.running_var\", \"backbone.body.layer3.2.conv3.weight\", \"backbone.body.layer3.2.bn3.weight\", \"backbone.body.layer3.2.bn3.bias\", \"backbone.body.layer3.2.bn3.running_mean\", \"backbone.body.layer3.2.bn3.running_var\", \"backbone.body.layer3.3.conv1.weight\", \"backbone.body.layer3.3.bn1.weight\", \"backbone.body.layer3.3.bn1.bias\", \"backbone.body.layer3.3.bn1.running_mean\", \"backbone.body.layer3.3.bn1.running_var\", \"backbone.body.layer3.3.conv2.weight\", \"backbone.body.layer3.3.bn2.weight\", \"backbone.body.layer3.3.bn2.bias\", \"backbone.body.layer3.3.bn2.running_mean\", \"backbone.body.layer3.3.bn2.running_var\", \"backbone.body.layer3.3.conv3.weight\", \"backbone.body.layer3.3.bn3.weight\", \"backbone.body.layer3.3.bn3.bias\", \"backbone.body.layer3.3.bn3.running_mean\", \"backbone.body.layer3.3.bn3.running_var\", \"backbone.body.layer3.4.conv1.weight\", \"backbone.body.layer3.4.bn1.weight\", \"backbone.body.layer3.4.bn1.bias\", \"backbone.body.layer3.4.bn1.running_mean\", \"backbone.body.layer3.4.bn1.running_var\", \"backbone.body.layer3.4.conv2.weight\", \"backbone.body.layer3.4.bn2.weight\", \"backbone.body.layer3.4.bn2.bias\", \"backbone.body.layer3.4.bn2.running_mean\", \"backbone.body.layer3.4.bn2.running_var\", \"backbone.body.layer3.4.conv3.weight\", \"backbone.body.layer3.4.bn3.weight\", \"backbone.body.layer3.4.bn3.bias\", \"backbone.body.layer3.4.bn3.running_mean\", \"backbone.body.layer3.4.bn3.running_var\", \"backbone.body.layer3.5.conv1.weight\", \"backbone.body.layer3.5.bn1.weight\", \"backbone.body.layer3.5.bn1.bias\", \"backbone.body.layer3.5.bn1.running_mean\", \"backbone.body.layer3.5.bn1.running_var\", \"backbone.body.layer3.5.conv2.weight\", \"backbone.body.layer3.5.bn2.weight\", \"backbone.body.layer3.5.bn2.bias\", \"backbone.body.layer3.5.bn2.running_mean\", \"backbone.body.layer3.5.bn2.running_var\", \"backbone.body.layer3.5.conv3.weight\", \"backbone.body.layer3.5.bn3.weight\", \"backbone.body.layer3.5.bn3.bias\", \"backbone.body.layer3.5.bn3.running_mean\", \"backbone.body.layer3.5.bn3.running_var\", \"backbone.body.layer4.0.conv1.weight\", \"backbone.body.layer4.0.bn1.weight\", \"backbone.body.layer4.0.bn1.bias\", \"backbone.body.layer4.0.bn1.running_mean\", \"backbone.body.layer4.0.bn1.running_var\", \"backbone.body.layer4.0.conv2.weight\", \"backbone.body.layer4.0.bn2.weight\", \"backbone.body.layer4.0.bn2.bias\", \"backbone.body.layer4.0.bn2.running_mean\", \"backbone.body.layer4.0.bn2.running_var\", \"backbone.body.layer4.0.conv3.weight\", \"backbone.body.layer4.0.bn3.weight\", \"backbone.body.layer4.0.bn3.bias\", \"backbone.body.layer4.0.bn3.running_mean\", \"backbone.body.layer4.0.bn3.running_var\", \"backbone.body.layer4.0.downsample.0.weight\", \"backbone.body.layer4.0.downsample.1.weight\", \"backbone.body.layer4.0.downsample.1.bias\", \"backbone.body.layer4.0.downsample.1.running_mean\", \"backbone.body.layer4.0.downsample.1.running_var\", \"backbone.body.layer4.1.conv1.weight\", \"backbone.body.layer4.1.bn1.weight\", \"backbone.body.layer4.1.bn1.bias\", \"backbone.body.layer4.1.bn1.running_mean\", \"backbone.body.layer4.1.bn1.running_var\", \"backbone.body.layer4.1.conv2.weight\", \"backbone.body.layer4.1.bn2.weight\", \"backbone.body.layer4.1.bn2.bias\", \"backbone.body.layer4.1.bn2.running_mean\", \"backbone.body.layer4.1.bn2.running_var\", \"backbone.body.layer4.1.conv3.weight\", \"backbone.body.layer4.1.bn3.weight\", \"backbone.body.layer4.1.bn3.bias\", \"backbone.body.layer4.1.bn3.running_mean\", \"backbone.body.layer4.1.bn3.running_var\", \"backbone.body.layer4.2.conv1.weight\", \"backbone.body.layer4.2.bn1.weight\", \"backbone.body.layer4.2.bn1.bias\", \"backbone.body.layer4.2.bn1.running_mean\", \"backbone.body.layer4.2.bn1.running_var\", \"backbone.body.layer4.2.conv2.weight\", \"backbone.body.layer4.2.bn2.weight\", \"backbone.body.layer4.2.bn2.bias\", \"backbone.body.layer4.2.bn2.running_mean\", \"backbone.body.layer4.2.bn2.running_var\", \"backbone.body.layer4.2.conv3.weight\", \"backbone.body.layer4.2.bn3.weight\", \"backbone.body.layer4.2.bn3.bias\", \"backbone.body.layer4.2.bn3.running_mean\", \"backbone.body.layer4.2.bn3.running_var\", \"backbone.fpn.inner_blocks.0.0.weight\", \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.weight\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.weight\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.weight\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.weight\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.weight\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.weight\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.weight\", \"backbone.fpn.layer_blocks.3.0.bias\", \"rpn.head.conv.0.0.weight\", \"rpn.head.conv.0.0.bias\", \"rpn.head.cls_logits.weight\", \"rpn.head.cls_logits.bias\", \"rpn.head.bbox_pred.weight\", \"rpn.head.bbox_pred.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\", \"roi_heads.box_predictor.cls_score.weight\", \"roi_heads.box_predictor.cls_score.bias\", \"roi_heads.box_predictor.bbox_pred.weight\", \"roi_heads.box_predictor.bbox_pred.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"best_fitness\", \"model\", \"ema\", \"updates\", \"optimizer\", \"train_args\", \"date\", \"version\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m model_new\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.body.conv1.weight\", \"backbone.body.bn1.weight\", \"backbone.body.bn1.bias\", \"backbone.body.bn1.running_mean\", \"backbone.body.bn1.running_var\", \"backbone.body.layer1.0.conv1.weight\", \"backbone.body.layer1.0.bn1.weight\", \"backbone.body.layer1.0.bn1.bias\", \"backbone.body.layer1.0.bn1.running_mean\", \"backbone.body.layer1.0.bn1.running_var\", \"backbone.body.layer1.0.conv2.weight\", \"backbone.body.layer1.0.bn2.weight\", \"backbone.body.layer1.0.bn2.bias\", \"backbone.body.layer1.0.bn2.running_mean\", \"backbone.body.layer1.0.bn2.running_var\", \"backbone.body.layer1.0.conv3.weight\", \"backbone.body.layer1.0.bn3.weight\", \"backbone.body.layer1.0.bn3.bias\", \"backbone.body.layer1.0.bn3.running_mean\", \"backbone.body.layer1.0.bn3.running_var\", \"backbone.body.layer1.0.downsample.0.weight\", \"backbone.body.layer1.0.downsample.1.weight\", \"backbone.body.layer1.0.downsample.1.bias\", \"backbone.body.layer1.0.downsample.1.running_mean\", \"backbone.body.layer1.0.downsample.1.running_var\", \"backbone.body.layer1.1.conv1.weight\", \"backbone.body.layer1.1.bn1.weight\", \"backbone.body.layer1.1.bn1.bias\", \"backbone.body.layer1.1.bn1.running_mean\", \"backbone.body.layer1.1.bn1.running_var\", \"backbone.body.layer1.1.conv2.weight\", \"backbone.body.layer1.1.bn2.weight\", \"backbone.body.layer1.1.bn2.bias\", \"backbone.body.layer1.1.bn2.running_mean\", \"backbone.body.layer1.1.bn2.running_var\", \"backbone.body.layer1.1.conv3.weight\", \"backbone.body.layer1.1.bn3.weight\", \"backbone.body.layer1.1.bn3.bias\", \"backbone.body.layer1.1.bn3.running_mean\", \"backbone.body.layer1.1.bn3.running_var\", \"backbone.body.layer1.2.conv1.weight\", \"backbone.body.layer1.2.bn1.weight\", \"backbone.body.layer1.2.bn1.bias\", \"backbone.body.layer1.2.bn1.running_mean\", \"backbone.body.layer1.2.bn1.running_var\", \"backbone.body.layer1.2.conv2.weight\", \"backbone.body.layer1.2.bn2.weight\", \"backbone.body.layer1.2.bn2.bias\", \"backbone.body.layer1.2.bn2.running_mean\", \"backbone.body.layer1.2.bn2.running_var\", \"backbone.body.layer1.2.conv3.weight\", \"backbone.body.layer1.2.bn3.weight\", \"backbone.body.layer1.2.bn3.bias\", \"backbone.body.layer1.2.bn3.running_mean\", \"backbone.body.layer1.2.bn3.running_var\", \"backbone.body.layer2.0.conv1.weight\", \"backbone.body.layer2.0.bn1.weight\", \"backbone.body.layer2.0.bn1.bias\", \"backbone.body.layer2.0.bn1.running_mean\", \"backbone.body.layer2.0.bn1.running_var\", \"backbone.body.layer2.0.conv2.weight\", \"backbone.body.layer2.0.bn2.weight\", \"backbone.body.layer2.0.bn2.bias\", \"backbone.body.layer2.0.bn2.running_mean\", \"backbone.body.layer2.0.bn2.running_var\", \"backbone.body.layer2.0.conv3.weight\", \"backbone.body.layer2.0.bn3.weight\", \"backbone.body.layer2.0.bn3.bias\", \"backbone.body.layer2.0.bn3.running_mean\", \"backbone.body.layer2.0.bn3.running_var\", \"backbone.body.layer2.0.downsample.0.weight\", \"backbone.body.layer2.0.downsample.1.weight\", \"backbone.body.layer2.0.downsample.1.bias\", \"backbone.body.layer2.0.downsample.1.running_mean\", \"backbone.body.layer2.0.downsample.1.running_var\", \"backbone.body.layer2.1.conv1.weight\", \"backbone.body.layer2.1.bn1.weight\", \"backbone.body.layer2.1.bn1.bias\", \"backbone.body.layer2.1.bn1.running_mean\", \"backbone.body.layer2.1.bn1.running_var\", \"backbone.body.layer2.1.conv2.weight\", \"backbone.body.layer2.1.bn2.weight\", \"backbone.body.layer2.1.bn2.bias\", \"backbone.body.layer2.1.bn2.running_mean\", \"backbone.body.layer2.1.bn2.running_var\", \"backbone.body.layer2.1.conv3.weight\", \"backbone.body.layer2.1.bn3.weight\", \"backbone.body.layer2.1.bn3.bias\", \"backbone.body.layer2.1.bn3.running_mean\", \"backbone.body.layer2.1.bn3.running_var\", \"backbone.body.layer2.2.conv1.weight\", \"backbone.body.layer2.2.bn1.weight\", \"backbone.body.layer2.2.bn1.bias\", \"backbone.body.layer2.2.bn1.running_mean\", \"backbone.body.layer2.2.bn1.running_var\", \"backbone.body.layer2.2.conv2.weight\", \"backbone.body.layer2.2.bn2.weight\", \"backbone.body.layer2.2.bn2.bias\", \"backbone.body.layer2.2.bn2.running_mean\", \"backbone.body.layer2.2.bn2.running_var\", \"backbone.body.layer2.2.conv3.weight\", \"backbone.body.layer2.2.bn3.weight\", \"backbone.body.layer2.2.bn3.bias\", \"backbone.body.layer2.2.bn3.running_mean\", \"backbone.body.layer2.2.bn3.running_var\", \"backbone.body.layer2.3.conv1.weight\", \"backbone.body.layer2.3.bn1.weight\", \"backbone.body.layer2.3.bn1.bias\", \"backbone.body.layer2.3.bn1.running_mean\", \"backbone.body.layer2.3.bn1.running_var\", \"backbone.body.layer2.3.conv2.weight\", \"backbone.body.layer2.3.bn2.weight\", \"backbone.body.layer2.3.bn2.bias\", \"backbone.body.layer2.3.bn2.running_mean\", \"backbone.body.layer2.3.bn2.running_var\", \"backbone.body.layer2.3.conv3.weight\", \"backbone.body.layer2.3.bn3.weight\", \"backbone.body.layer2.3.bn3.bias\", \"backbone.body.layer2.3.bn3.running_mean\", \"backbone.body.layer2.3.bn3.running_var\", \"backbone.body.layer3.0.conv1.weight\", \"backbone.body.layer3.0.bn1.weight\", \"backbone.body.layer3.0.bn1.bias\", \"backbone.body.layer3.0.bn1.running_mean\", \"backbone.body.layer3.0.bn1.running_var\", \"backbone.body.layer3.0.conv2.weight\", \"backbone.body.layer3.0.bn2.weight\", \"backbone.body.layer3.0.bn2.bias\", \"backbone.body.layer3.0.bn2.running_mean\", \"backbone.body.layer3.0.bn2.running_var\", \"backbone.body.layer3.0.conv3.weight\", \"backbone.body.layer3.0.bn3.weight\", \"backbone.body.layer3.0.bn3.bias\", \"backbone.body.layer3.0.bn3.running_mean\", \"backbone.body.layer3.0.bn3.running_var\", \"backbone.body.layer3.0.downsample.0.weight\", \"backbone.body.layer3.0.downsample.1.weight\", \"backbone.body.layer3.0.downsample.1.bias\", \"backbone.body.layer3.0.downsample.1.running_mean\", \"backbone.body.layer3.0.downsample.1.running_var\", \"backbone.body.layer3.1.conv1.weight\", \"backbone.body.layer3.1.bn1.weight\", \"backbone.body.layer3.1.bn1.bias\", \"backbone.body.layer3.1.bn1.running_mean\", \"backbone.body.layer3.1.bn1.running_var\", \"backbone.body.layer3.1.conv2.weight\", \"backbone.body.layer3.1.bn2.weight\", \"backbone.body.layer3.1.bn2.bias\", \"backbone.body.layer3.1.bn2.running_mean\", \"backbone.body.layer3.1.bn2.running_var\", \"backbone.body.layer3.1.conv3.weight\", \"backbone.body.layer3.1.bn3.weight\", \"backbone.body.layer3.1.bn3.bias\", \"backbone.body.layer3.1.bn3.running_mean\", \"backbone.body.layer3.1.bn3.running_var\", \"backbone.body.layer3.2.conv1.weight\", \"backbone.body.layer3.2.bn1.weight\", \"backbone.body.layer3.2.bn1.bias\", \"backbone.body.layer3.2.bn1.running_mean\", \"backbone.body.layer3.2.bn1.running_var\", \"backbone.body.layer3.2.conv2.weight\", \"backbone.body.layer3.2.bn2.weight\", \"backbone.body.layer3.2.bn2.bias\", \"backbone.body.layer3.2.bn2.running_mean\", \"backbone.body.layer3.2.bn2.running_var\", \"backbone.body.layer3.2.conv3.weight\", \"backbone.body.layer3.2.bn3.weight\", \"backbone.body.layer3.2.bn3.bias\", \"backbone.body.layer3.2.bn3.running_mean\", \"backbone.body.layer3.2.bn3.running_var\", \"backbone.body.layer3.3.conv1.weight\", \"backbone.body.layer3.3.bn1.weight\", \"backbone.body.layer3.3.bn1.bias\", \"backbone.body.layer3.3.bn1.running_mean\", \"backbone.body.layer3.3.bn1.running_var\", \"backbone.body.layer3.3.conv2.weight\", \"backbone.body.layer3.3.bn2.weight\", \"backbone.body.layer3.3.bn2.bias\", \"backbone.body.layer3.3.bn2.running_mean\", \"backbone.body.layer3.3.bn2.running_var\", \"backbone.body.layer3.3.conv3.weight\", \"backbone.body.layer3.3.bn3.weight\", \"backbone.body.layer3.3.bn3.bias\", \"backbone.body.layer3.3.bn3.running_mean\", \"backbone.body.layer3.3.bn3.running_var\", \"backbone.body.layer3.4.conv1.weight\", \"backbone.body.layer3.4.bn1.weight\", \"backbone.body.layer3.4.bn1.bias\", \"backbone.body.layer3.4.bn1.running_mean\", \"backbone.body.layer3.4.bn1.running_var\", \"backbone.body.layer3.4.conv2.weight\", \"backbone.body.layer3.4.bn2.weight\", \"backbone.body.layer3.4.bn2.bias\", \"backbone.body.layer3.4.bn2.running_mean\", \"backbone.body.layer3.4.bn2.running_var\", \"backbone.body.layer3.4.conv3.weight\", \"backbone.body.layer3.4.bn3.weight\", \"backbone.body.layer3.4.bn3.bias\", \"backbone.body.layer3.4.bn3.running_mean\", \"backbone.body.layer3.4.bn3.running_var\", \"backbone.body.layer3.5.conv1.weight\", \"backbone.body.layer3.5.bn1.weight\", \"backbone.body.layer3.5.bn1.bias\", \"backbone.body.layer3.5.bn1.running_mean\", \"backbone.body.layer3.5.bn1.running_var\", \"backbone.body.layer3.5.conv2.weight\", \"backbone.body.layer3.5.bn2.weight\", \"backbone.body.layer3.5.bn2.bias\", \"backbone.body.layer3.5.bn2.running_mean\", \"backbone.body.layer3.5.bn2.running_var\", \"backbone.body.layer3.5.conv3.weight\", \"backbone.body.layer3.5.bn3.weight\", \"backbone.body.layer3.5.bn3.bias\", \"backbone.body.layer3.5.bn3.running_mean\", \"backbone.body.layer3.5.bn3.running_var\", \"backbone.body.layer4.0.conv1.weight\", \"backbone.body.layer4.0.bn1.weight\", \"backbone.body.layer4.0.bn1.bias\", \"backbone.body.layer4.0.bn1.running_mean\", \"backbone.body.layer4.0.bn1.running_var\", \"backbone.body.layer4.0.conv2.weight\", \"backbone.body.layer4.0.bn2.weight\", \"backbone.body.layer4.0.bn2.bias\", \"backbone.body.layer4.0.bn2.running_mean\", \"backbone.body.layer4.0.bn2.running_var\", \"backbone.body.layer4.0.conv3.weight\", \"backbone.body.layer4.0.bn3.weight\", \"backbone.body.layer4.0.bn3.bias\", \"backbone.body.layer4.0.bn3.running_mean\", \"backbone.body.layer4.0.bn3.running_var\", \"backbone.body.layer4.0.downsample.0.weight\", \"backbone.body.layer4.0.downsample.1.weight\", \"backbone.body.layer4.0.downsample.1.bias\", \"backbone.body.layer4.0.downsample.1.running_mean\", \"backbone.body.layer4.0.downsample.1.running_var\", \"backbone.body.layer4.1.conv1.weight\", \"backbone.body.layer4.1.bn1.weight\", \"backbone.body.layer4.1.bn1.bias\", \"backbone.body.layer4.1.bn1.running_mean\", \"backbone.body.layer4.1.bn1.running_var\", \"backbone.body.layer4.1.conv2.weight\", \"backbone.body.layer4.1.bn2.weight\", \"backbone.body.layer4.1.bn2.bias\", \"backbone.body.layer4.1.bn2.running_mean\", \"backbone.body.layer4.1.bn2.running_var\", \"backbone.body.layer4.1.conv3.weight\", \"backbone.body.layer4.1.bn3.weight\", \"backbone.body.layer4.1.bn3.bias\", \"backbone.body.layer4.1.bn3.running_mean\", \"backbone.body.layer4.1.bn3.running_var\", \"backbone.body.layer4.2.conv1.weight\", \"backbone.body.layer4.2.bn1.weight\", \"backbone.body.layer4.2.bn1.bias\", \"backbone.body.layer4.2.bn1.running_mean\", \"backbone.body.layer4.2.bn1.running_var\", \"backbone.body.layer4.2.conv2.weight\", \"backbone.body.layer4.2.bn2.weight\", \"backbone.body.layer4.2.bn2.bias\", \"backbone.body.layer4.2.bn2.running_mean\", \"backbone.body.layer4.2.bn2.running_var\", \"backbone.body.layer4.2.conv3.weight\", \"backbone.body.layer4.2.bn3.weight\", \"backbone.body.layer4.2.bn3.bias\", \"backbone.body.layer4.2.bn3.running_mean\", \"backbone.body.layer4.2.bn3.running_var\", \"backbone.fpn.inner_blocks.0.0.weight\", \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.weight\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.weight\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.weight\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.weight\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.weight\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.weight\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.weight\", \"backbone.fpn.layer_blocks.3.0.bias\", \"rpn.head.conv.0.0.weight\", \"rpn.head.conv.0.0.bias\", \"rpn.head.cls_logits.weight\", \"rpn.head.cls_logits.bias\", \"rpn.head.bbox_pred.weight\", \"rpn.head.bbox_pred.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\", \"roi_heads.box_predictor.cls_score.weight\", \"roi_heads.box_predictor.cls_score.bias\", \"roi_heads.box_predictor.bbox_pred.weight\", \"roi_heads.box_predictor.bbox_pred.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"best_fitness\", \"model\", \"ema\", \"updates\", \"optimizer\", \"train_args\", \"date\", \"version\". "
     ]
    }
   ],
   "source": [
    "model =\"best.pt\"\n",
    "\n",
    "\n",
    "\n",
    "model_new.load_state_dict(torch.load(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ade443e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'backbone'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m new_detection_head_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(new_weights_path)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Access the necessary components from the loaded models\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m original_model_backbone \u001b[38;5;241m=\u001b[39m original_model_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace with the correct attribute name\u001b[39;00m\n\u001b[0;32m     14\u001b[0m new_detection_head \u001b[38;5;241m=\u001b[39m new_detection_head_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhead\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace with the correct attribute name\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a new YOLO model with the same backbone as the original model\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'backbone'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the original YOLO model with 80 classes\n",
    "original_model_path = \"yolov8s (2) (1).pt\"  # Replace with the path to the original model\n",
    "original_model_dict = torch.load(original_model_path)\n",
    "\n",
    "# Load the weights of the detection head for the new 5 classes\n",
    "new_weights_path = \"best.pt\"  # Replace with the path to the new weights\n",
    "new_detection_head_dict = torch.load(new_weights_path)\n",
    "\n",
    "# Access the necessary components from the loaded models\n",
    "original_model_backbone = original_model_dict['backbone']  # Replace with the correct attribute name\n",
    "new_detection_head = new_detection_head_dict['head']  # Replace with the correct attribute name\n",
    "\n",
    "# Create a new YOLO model with the same backbone as the original model\n",
    "combined_model = nn.Sequential(original_model_backbone)\n",
    "\n",
    "# Add the detection head for the new classes\n",
    "combined_model[0].head = new_detection_head\n",
    "\n",
    "# Now you can use the combined_model for your specific task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55ab42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81828ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
